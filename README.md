# codellama-7b
This repo explains how the CodeLlama-7b can be run with a 4GB NVIDIA GPU.

1. Get transformers:
  ```pip install git+https://github.com/huggingface/transformers.git@main accelerate```

2. Install CUDA, preferably 11.7 or 11.8 version. 

3. If an error "PackageNotFoundError: No package metadata was found for bitsandbytes" comes up, the solution for me was:
```pip install bitsandbytes-cuda117 bitsandbytes```
Also might need to install scipy and accelerate:
```pip install scipy```
```pip install accelerate```

4. Run codellama.py. This code uses quantization in order to run a bit faster. The device_map was tuned by running the code some times, the reference values where generated by deviceMap.py which returns a map to run without quantization.



References:
1)https://huggingface.co/docs/transformers/main/en/main_classes/quantization#advanced-use-cases
2)https://huggingface.co/codellama/CodeLlama-7b-Instruct-hf
3)https://huggingface.co/blog/codellama
